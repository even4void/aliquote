---
title: "Tech Review June 2018"
date: 2018-06-18T21:40:51+02:00
lastmod: 2018-06-18T21:40:51+02:00
draft: true
description: ""
tags: []
categories: ["tech-review"]
---

The monthly tech newsletter is finally out!

<!--more-->

Besides some [Essential cheat sheets for machine learning and deep learning engineers](https://www.sodavision.com/essential-cheat-sheets-for-machine-learning-and-deep-learning-engineers/), the second part of the very [visual introduction to Machine Learning](http://www.r2d3.us/visual-intro-to-machine-learning-part-2/) was released a few days ago. Check it out! In the same spirit, but without interactive display, there is <http://explained.ai>, maintained by Terence Parr who is currently working on [The Mechanics of Machine Learning](https://mlbook.explained.ai). With the advances of JS backend for [Tensorflow](https://js.tensorflow.org) and [ML](https://ml5js.org), I anticipate more online interactive tutorials coming soon.

Two new books on my viewfinder: Bernhard Schölkopf is probably well known for his many books on SVM and [kernel methods](http://agbs.kyb.tuebingen.mpg.de/lwk/), and there is now an open-access MIT book on causality: [Elements of Causal Inference, Foundations and Learning Algorithms](https://mitpress.mit.edu/books/elements-causal-inference); [Introduction to Theoretical Computer Science](http://introtcs.org/public/index.html), by Boaz Barak.

If you are looking for some datasets, here are the (supposed) "[50 Best Free Datasets for Machine Learning](https://gengo.ai/articles/the-50-best-free-datasets-for-machine-learning/)", although I remain circumspect as to the following assertions: a dataset shouldn’t be messy, because you don’t want to spend a lot of time cleaning data; a dataset shouldn’t have too many rows or columns, so it’s easy to work with. Aren't we spending almost 80% of our time cleaning data in the real life? So, why not start with a very dirty dataset and apply data cleansing techniques and feature engineering like a big boss?

[LISP: back to the future (a tribute to 60th anniversary)](https://sigma.software/about/media/lisp-back-future-tribute-60th-anniversary). Because why not?

I learned about [Fathom](https://usefathom.com) and [Caddy](https://caddyserver.com) on Jack Baty's blog. Currently, I am using Matomo to monitor the stats of this website. Like Jack Baty, I don't really care about engagement and related analytics and I know that there are mostly "volatile" visitors on aliquote.org. Still, I find it interesting to know where people spend their free time, whether on this blog or on static pages. Most of the traffic has always been directed to my teaching materials

It's been a while since I was writing Awk and Sed scripts every two or three days to inspect and reshape data files or any sort of STDOUT. Hence, I was happy to find this [little intro on Awk](https://gregable.com/2010/09/why-you-should-know-just-little-awk.html) pop up in my Twitter timeline. On a related note, while I find myself comfortable with using Emacs all day long, I'm always fascinated by [those who build their work environment around](http://doc.rix.si/cce/cce.html).

> I honestly don’t know if Racket and Haskell are worth their costs in complexity. At the end of the day, maybe what really matters is writing simple, consistent things that other people can understand.
>
> -- Alexis King on [lexi-lambda](https://lexi-lambda.github.io/blog/2016/08/11/climbing-the-infinite-ladder-of-abstraction/)

MXNet is incubating as an Apache project and is yet another library for deep learning. Carin Meier is currently working on a [Clojure package](https://github.com/gigasquid/clojure-mxnet). See her [recent blog post](http://gigasquid.github.io/blog/2018/06/03/meet-clojure-mxnet-ndarray/) to learn more.
